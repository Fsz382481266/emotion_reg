{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mean_max_pool_bertwwmext.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L135ck1JqZP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "afe08b1c-125f-4942-a8e4-124c9187f51c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr  5 18:53:48 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bmUkrCuq0qZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "os.chdir('drive/My Drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOK07fRXq_9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "b3b0e3a1-5b1b-4df4-d726-a727defa7b69"
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.12.33)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.15.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.33->boto3->pytorch_pretrained_bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-pQWSt3rJrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from tqdm import tqdm\n",
        "import torch.utils.data.dataloader as dataloader\n",
        "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
        "from pytorch_pretrained_bert import BertForSequenceClassification,BertModel\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynj7EHGOrNex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 140\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "input_categories = '微博中文内容'\n",
        "output_categories = '情感倾向'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goli6xvtrUf0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "351c6aa8-ff79-4897-ee11-efed9d88f8f5"
      },
      "source": [
        "train_0 = pd.read_csv(\"data/train_6\")\n",
        "val_0 = pd.read_csv(\"data/val_6\")\n",
        "print(train_0.shape,val_0.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(89913, 7) (10000, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XQ-2tbd3Wij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_0 = train_0[:256]\n",
        "val_0 = val_0[:256]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFZoci6xrbOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _convert_to_transformer_inputs(instance,tokenizer,max_sequence_length):\n",
        "    def return_id(str1,truncation_strategy,length):\n",
        "        inputs = tokenizer.tokenize(str1)\n",
        "        if len(inputs) > 138:\n",
        "          inputs = inputs[:138]\n",
        "        inputs = [\"[CLS]\"]+ inputs + [\"[SEP]\"]\n",
        "        input_ids =  tokenizer.convert_tokens_to_ids(inputs)\n",
        "#         print(input_ids)\n",
        "        input_masks = [1] * len(input_ids)\n",
        "#         print(input_masks)\n",
        "        input_segments = [0] * len(input_ids)\n",
        "        padding_length = length - len(input_ids)\n",
        "#         padding_id = tokenizer.pad_token_id\n",
        "        input_ids = input_ids + ([0] * padding_length)\n",
        "        input_masks = input_masks + ([0] * padding_length)\n",
        "        input_segments = input_segments + ([0] * padding_length)\n",
        "        # if len(input_ids) != 200:\n",
        "        #   print(str1,len(input_ids))\n",
        "        return [input_ids, input_masks, input_segments]\n",
        "    \n",
        "    input_ids,input_masks,input_segments = return_id(instance, 'longest_first', max_sequence_length)\n",
        "    return [input_ids, input_masks,input_segments]\n",
        "\n",
        "\n",
        "def compute_input_arrays(df,columns,tokenizer,max_sequence_length):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for instance in tqdm(df[columns]):\n",
        "        \n",
        "        ids, masks, segments = \\\n",
        "        _convert_to_transformer_inputs(str(instance), tokenizer, max_sequence_length)\n",
        "        \n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "    # print(input_ids)\n",
        "\n",
        "    return input_ids, input_masks, input_segments\n",
        "\n",
        "def compute_output_arrays(df,columns):\n",
        "    return np.asarray(df[columns].astype(int) + 1)\n",
        "\n",
        "def data_loader(input_ids,input_masks,input_segments,label_ids):\n",
        "    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "    all_input_mask = torch.tensor(input_masks, dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor(input_segments, dtype=torch.long)\n",
        "    all_label = torch.tensor(label_ids, dtype=torch.long)\n",
        "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n",
        "    return train_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UUEepgyrg5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(\"bert_wwm_ext/\")\n",
        "    self.dense = nn.Linear(in_features = 768*2,out_features =768)\n",
        "    # 64*64*3,  1\n",
        "    self.final_dense = nn.Linear(768,3)\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "    # for param in self.bert.parameters():\n",
        "    #   param.requires_grad = True \n",
        "    # self.fc = nn.Linear(768,3)\n",
        "  \n",
        "  # def computer_loss(self,predictions,labels):\n",
        "  #   predictions = predictions.view(-1)\n",
        "  #   labels = labels.float().view(-1)\n",
        "  #   epsilon = 1e-8\n",
        "  #   loss = \\\n",
        "  #       - labels * torch.log(predictions + epsilon) - \\\n",
        "  #       (torch.tensor(1.0)-labels) * torch.log(torch.tensor())\n",
        "\n",
        "  def forward(self,input_ids,input_mask,labels=None):\n",
        "    # \n",
        "    encoded_layers, _ = self.bert(input_ids,input_mask)\n",
        "    sequence_output = encoded_layers[5]\n",
        "    avg_pooled = sequence_output.mean(1)\n",
        "    max_pooled = torch.max(sequence_output,dim=1)\n",
        "    pooled = torch.cat((avg_pooled,max_pooled[0]),dim=1)\n",
        "\n",
        "    pooled = self.dense(pooled)\n",
        "    predictions = self.final_dense(pooled)\n",
        "    return predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSXBJmYPzIcI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "47954ce8-1381-4035-a496-6d4ced316a9f"
      },
      "source": [
        "tokenizer = BertTokenizer(vocab_file=\"bert_wwm_ext/vocab.txt\") \n",
        "t_input_ids, t_input_masks, t_input_segments = compute_input_arrays(train_0,input_categories,tokenizer,MAX_SEQUENCE_LENGTH)\n",
        "print(len(t_input_segments))\n",
        "t_label_ids = compute_output_arrays(train_0, output_categories)\n",
        "train_dataloader = data_loader(t_input_ids, t_input_masks, t_input_segments,t_label_ids)\n",
        "\n",
        "v_input_ids, v_input_masks, v_input_segments = compute_input_arrays(val_0,input_categories,tokenizer,MAX_SEQUENCE_LENGTH)\n",
        "v_label_ids = compute_output_arrays(val_0, output_categories)\n",
        "val_dataloader = data_loader(v_input_ids, v_input_masks, v_input_segments,v_label_ids)\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "# bert_model = BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path='bert-base-chinese', num_labels=3)\n",
        "bert_model = Model().to(device)\n",
        "# param_optimizer = list(bert_model.named_parameters())       \n",
        "# param_optimizer = [n for n in param_optimizer]\n",
        "# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "# optimizer_grouped_parameters = [\n",
        "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "#     ]\n",
        "param_optimizer = list(bert_model.named_parameters())  # 模型参数名字列表\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.01}]\n",
        "NUM_EPOCHS =8\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=0.05,\n",
        "                     t_total=len(train_0) * NUM_EPOCHS)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# bert_model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 89913/89913 [00:43<00:00, 2045.62it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "89913\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:04<00:00, 2138.08it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSZIF3_59kCy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad31cbd0-b748-4702-b868-fb6bf0f5b08a"
      },
      "source": [
        "test_df = pd.read_csv('nCov_10k_test.csv',header=0)\n",
        "# test_df = pd.read_csv('nCov_10k_test.csv',header=0)\n",
        "test_df1 = test_df.copy()\n",
        "# test_df1.info()\n",
        "dev_input_ids, dev_input_masks, dev_input_segments = compute_input_arrays(test_df1,input_categories,tokenizer,MAX_SEQUENCE_LENGTH)\n",
        "def test_loader(input_ids,input_masks,input_segments):\n",
        "    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "    all_input_mask = torch.tensor(input_masks, dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor(input_segments, dtype=torch.long)\n",
        "    \n",
        "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    return train_dataloader\n",
        "\n",
        "dev_dataloader = test_loader(dev_input_ids, dev_input_masks, dev_input_segments)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:05<00:00, 1949.37it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GRkp5Ba92s_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.metrics import f1_score\n",
        "class OptimizedF1(object):\n",
        "    def __init__(self):\n",
        "        self.coef_ = []\n",
        "\n",
        "    def _kappa_loss(self, coef, X, y):\n",
        "        \"\"\"\n",
        "        y_hat = argmax(coef*X, axis=-1)\n",
        "        :param coef: (1D array) weights\n",
        "        :param X: (2D array)logits\n",
        "        :param y: (1D array) label\n",
        "        :return: -f1\n",
        "        \"\"\"\n",
        "        # print(\"X:\",X)\n",
        "        X_p = np.copy(X)\n",
        "        # print(\"X_p:\",type(X_p),\"coef:\",coef)\n",
        "        X_p = X_p*coef\n",
        "        ll = f1_score(y, np.argmax(X_p, axis=-1), average='macro')\n",
        "        return -ll\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
        "        initial_coef = [1. for _ in range(3)]\n",
        "        # print(\"initial_coef :\",initial_coef)\n",
        "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
        "\n",
        "    def predict(self, X, y):\n",
        "        X_p = np.copy(X)\n",
        "        X_p = self.coef_['x'] * X_p\n",
        "        print(\"X_p:\",X_p,\"f1:\",f1_score(y, np.argmax(X_p, axis=-1), average='macro'))\n",
        "        return f1_score(y, np.argmax(X_p, axis=-1), average='macro')\n",
        "\n",
        "    def coefficients(self):\n",
        "        return self.coef_['x']\n",
        "\n",
        "    def prt_coef(self):\n",
        "        am= self.coef_\n",
        "        print(am)\n",
        "op = OptimizedF1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDEul0Ky97mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "from torch.autograd import Variable\n",
        "\n",
        "def change_csv(abblist):\n",
        "    abclist=[]\n",
        "    for i in range(157):\n",
        "      if i != 156 :\n",
        "        for j in range(64):\n",
        "            abclist.append(int(abblist[i][j])-1)\n",
        "      else:\n",
        "        for j in range(16):\n",
        "          abclist.append(int(abblist[i][j])-1)\n",
        "    dic1={}\n",
        "    for i in abclist:\n",
        "        dic1[i] = abclist.count(i)\n",
        "    print(dic1)\n",
        "    return abclist\n",
        "\n",
        "def create_test_csv(abclist):\n",
        "    df1_test = test_df1.copy()\n",
        "    df1_test['id']=df1_test[\"微博id\"]\n",
        "    df2_test_pud=df1_test.drop(labels=['微博id','微博发布时间',\"发布人账号\",'微博中文内容','微博图片','微博视频'],axis=1)\n",
        "    letters_test_pud = ['id']\n",
        "    df7_test=df2_test_pud[letters_test_pud]\n",
        "    df7_test['y']=None\n",
        "    df7_test['y'] = abclist\n",
        "    # df_sub['id'] = df_sub['id'].apply(lambda x: str(x)+' ')\n",
        "    # df7_test.to_csv('test_03341.csv',index=False, encoding='utf-8')\n",
        "\n",
        "    return df7_test\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    i = 0\n",
        "    \n",
        "    for input_ids, segment_ids,input_mask,label_ids in iterator:\n",
        "        input_ids = input_ids.to(device)\n",
        "        input_mask = input_mask.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        label_ids = label_ids.to(device)\n",
        "        output = model(input_ids,segment_ids)\n",
        "        logits = output\n",
        "        logits2 = logits.cpu().detach()\n",
        "        y_pred_notrick=logits.argmax(dim=1).cpu()\n",
        "        # print(\"logits1:\",type(logits))\n",
        "        model.zero_grad()\n",
        "        # if i %600 == 0:\n",
        "        #     print(\"---未测试时coef的值---------------\")\n",
        "        #     op.prt_coef()\n",
        "        op.fit(logits2,label_ids.cpu())\n",
        "        tips = Variable(torch.Tensor(op.coefficients()),requires_grad=True)\n",
        "        # if i %600 == 0:\n",
        "        #     print(\"---测试后coef的值---------------\")\n",
        "        #     op.prt_coef()\n",
        "        \n",
        "        logits = tips*(logits.cpu())\n",
        "        y_pred_c = logits.argmax(dim=1).cpu()\n",
        "        logits = logits.to(device)\n",
        "        # print(\"logits2:\",logits.argmax(dim=1)) \n",
        "        # y_pred_label = y_pred.cpu()\n",
        "        loss = F.cross_entropy(logits, label_ids)\n",
        "        epoch_loss += loss.cpu()\n",
        "        # y_pred_c = y_pred.argmax(dim=1).cpu()\n",
        "        # print(y_pred_c)\n",
        "        label_ids_c = label_ids.cpu()\n",
        "        # print(label_ids_c)\n",
        "        if i %300== 0:\n",
        "          # op.prt_coef()\n",
        "          # print(y_pred_c)\n",
        "          # print(label_ids_c)\n",
        "          print(\"i\",i,\"loss\",loss.cpu(),\"train acc:\",accuracy_score(y_pred_c,label_ids_c),\"train rec:\",recall_score(y_pred_c,label_ids_c,average='macro'),\"train f1\",f1_score(y_pred_c,label_ids_c,average='macro'))\n",
        "        loss.backward()\n",
        "        optimizer.step() \n",
        "        i += 1\n",
        "    end = time.time()\n",
        "    runtime = end-start\n",
        "    print('time: %.2f' , runtime)\n",
        "    return epoch_loss / len(iterator)\n",
        "def deval(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    abblist = []\n",
        "    n = 0\n",
        "    f1 = 0\n",
        "    acc = 0\n",
        "    rec = 0\n",
        "    with torch.no_grad():\n",
        "      # print(\"----------pred time the coef:----------\")\n",
        "      # op.prt_coef()\n",
        "      for input_ids,  segment_ids,input_mask,label_ids in iterator:\n",
        "        \n",
        "        n += 1\n",
        "        input_ids = input_ids.to(device)\n",
        "        input_mask = input_mask.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        output = model(input_ids,segment_ids)\n",
        "\n",
        "        # logits2 = output.cpu().detach()\n",
        "        \n",
        "        # tips2 = Variable(torch.Tensor(op.coefficients()),requires_grad=True)\n",
        "        logits = output.cpu()\n",
        "        # print(\"-------------test-------------------\")\n",
        "        # print(\"label:\",output.argmax(dim=1).cpu())\n",
        "        # print(\"pred:\",logits.argmax(dim=1).cpu())\n",
        "        # print(\"-------------test-------------------\")\n",
        "        # op.prt_coef()\n",
        "        y_pred_label = logits.argmax(dim=1).cpu()\n",
        "        acc += accuracy_score(y_pred_label,label_ids)\n",
        "        rec += recall_score(y_pred_label,label_ids,average='macro')\n",
        "        f1 += f1_score(y_pred_label,label_ids,average='macro')\n",
        "      print(\"train acc :\",acc/n, \"rec:\",rec/n,\"f1:\",f1/n,'n:',n)\n",
        "def pred(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    abblist = []\n",
        "    abclist = []\n",
        "    with torch.no_grad():\n",
        "      for input_ids,  segment_ids,input_mask in iterator:\n",
        "          input_ids = input_ids.to(device)\n",
        "          input_mask = input_mask.to(device)\n",
        "          segment_ids = segment_ids.to(device)\n",
        "          output = model(input_ids, segment_ids)\n",
        "          # tips2 = Variable(torch.Tensor(op.coefficients()),requires_grad=True)\n",
        "          # logits = tips2*(output.cpu())\n",
        "          y_pred_label = output.argmax(dim=1).cpu()\n",
        "          abblist.append(y_pred_label)\n",
        "      abclist = change_csv(abblist)\n",
        "      test_csv = create_test_csv(abclist)\n",
        "    return test_csv\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bump7Fwy_ivL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67570833-1bad-40c8-f4cb-9c054797c9c4"
      },
      "source": [
        "stat = 0 \n",
        "for i in range(epochs):\n",
        "    train_loss = train(bert_model, train_dataloader, optimizer, criterion, device) \n",
        "    lss = \"mean_max_pool_berwwmext_model/p100_mmpbertwwmext_trick_\"+str(i)+\"_.pk1\"\n",
        "    if i == 0 :\n",
        "      stat = train_loss\n",
        "    if i != 0 :\n",
        "      if stat-train_loss < 0.005 :\n",
        "        break\n",
        "      if stat-train_loss < 0 :\n",
        "        break\n",
        "    torch.save(bert_model.state_dict(), lss)\n",
        "    print(\"train loss: \", train_loss)\n",
        "    deval(bert_model, val_dataloader, criterion, device)\n",
        "    test_csv = pred(bert_model, dev_dataloader, criterion, device)\n",
        "    test_csv.to_csv('mean_max_pool_berwwmext_model/test_mmpbertwwmext_'+str(i)+'.csv',index=False, encoding='utf-8')\n",
        "    # deval(bert_model, train_dataloader, criterion, device)\n",
        "# torch.save(xlnet_model.state_dict(), \"xlnet_model/p100_xlnet_trick_changedata_true_end.pk1\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "i 0 loss tensor(1.3814, grad_fn=<CopyBackwards>) train acc: 0.171875 train rec: 0.057291666666666664 train f1 0.09777777777777778\n",
            "i 300 loss tensor(1.0099, grad_fn=<CopyBackwards>) train acc: 0.515625 train rec: 0.171875 train f1 0.2268041237113402\n",
            "i 600 loss tensor(0.8455, grad_fn=<CopyBackwards>) train acc: 0.640625 train rec: 0.21354166666666666 train f1 0.26031746031746034\n",
            "i 900 loss tensor(0.8384, grad_fn=<CopyBackwards>) train acc: 0.640625 train rec: 0.21354166666666666 train f1 0.26031746031746034\n",
            "i 1200 loss tensor(0.8549, grad_fn=<CopyBackwards>) train acc: 0.609375 train rec: 0.4033898305084746 train f1 0.36764705882352944\n",
            "time: %.2f 925.6760890483856\n",
            "train loss:  tensor(0.9320, grad_fn=<DivBackward0>)\n",
            "train acc : 0.6921775477707006 rec: 0.6755410154936315 f1: 0.5922832819997295 n: 157\n",
            "{0: 7237, -1: 801, 1: 1962}\n",
            "i 0 loss tensor(0.5191, grad_fn=<CopyBackwards>) train acc: 0.796875 train rec: 0.6714975845410628 train f1 0.666056166056166\n",
            "i 300 loss tensor(0.6080, grad_fn=<CopyBackwards>) train acc: 0.703125 train rec: 0.7178571428571429 train f1 0.6585375118708451\n",
            "i 600 loss tensor(0.5870, grad_fn=<CopyBackwards>) train acc: 0.796875 train rec: 0.7592592592592592 train f1 0.723129867134976\n",
            "i 900 loss tensor(0.5560, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.7975867853916635 train f1 0.7710468153506128\n",
            "i 1200 loss tensor(0.4478, grad_fn=<CopyBackwards>) train acc: 0.796875 train rec: 0.71869918699187 train f1 0.6793495934959349\n",
            "time: %.2f 926.7641727924347\n",
            "train loss:  tensor(0.6370, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7326831210191083 rec: 0.7020182058106357 f1: 0.6903753812051604 n: 157\n",
            "{0: 5990, -1: 1513, 1: 2497}\n",
            "i 0 loss tensor(0.6752, grad_fn=<CopyBackwards>) train acc: 0.734375 train rec: 0.7720867208672088 train f1 0.6725322607675549\n",
            "i 300 loss tensor(0.7366, grad_fn=<CopyBackwards>) train acc: 0.6875 train rec: 0.6402116402116402 train f1 0.649493631562597\n",
            "i 600 loss tensor(0.7128, grad_fn=<CopyBackwards>) train acc: 0.71875 train rec: 0.7282051282051282 train f1 0.6931635192504757\n",
            "i 900 loss tensor(0.6556, grad_fn=<CopyBackwards>) train acc: 0.71875 train rec: 0.6914264597191426 train f1 0.6716033755274262\n",
            "i 1200 loss tensor(0.5566, grad_fn=<CopyBackwards>) train acc: 0.796875 train rec: 0.7931159420289854 train f1 0.7412601514234908\n",
            "time: %.2f 927.2483549118042\n",
            "train loss:  tensor(0.5899, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7436305732484076 rec: 0.716842384371135 f1: 0.6957694595692859 n: 157\n",
            "{0: 6212, -1: 1478, 1: 2310}\n",
            "i 0 loss tensor(0.5633, grad_fn=<CopyBackwards>) train acc: 0.765625 train rec: 0.7555555555555555 train f1 0.7519607843137255\n",
            "i 300 loss tensor(0.5406, grad_fn=<CopyBackwards>) train acc: 0.828125 train rec: 0.8783068783068783 train f1 0.7861501348091386\n",
            "i 600 loss tensor(0.7476, grad_fn=<CopyBackwards>) train acc: 0.703125 train rec: 0.6702317290552585 train f1 0.6565591397849462\n",
            "i 900 loss tensor(0.5718, grad_fn=<CopyBackwards>) train acc: 0.734375 train rec: 0.6924603174603176 train f1 0.6705901948809062\n",
            "i 1200 loss tensor(0.3899, grad_fn=<CopyBackwards>) train acc: 0.796875 train rec: 0.7826210826210825 train f1 0.7365163572060123\n",
            "time: %.2f 926.7520997524261\n",
            "train loss:  tensor(0.5634, grad_fn=<DivBackward0>)\n",
            "train acc : 0.747312898089172 rec: 0.7168202876982009 f1: 0.7072268315773894 n: 157\n",
            "{1: 2684, 0: 5813, -1: 1503}\n",
            "i 0 loss tensor(0.6817, grad_fn=<CopyBackwards>) train acc: 0.6875 train rec: 0.6538807189542483 train f1 0.6685812239809436\n",
            "i 300 loss tensor(0.4325, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.730701754385965 train f1 0.7457264957264957\n",
            "i 600 loss tensor(0.4405, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.8035008469791078 train f1 0.7678787878787879\n",
            "i 900 loss tensor(0.3765, grad_fn=<CopyBackwards>) train acc: 0.9375 train rec: 0.9521367521367522 train f1 0.9274836266077489\n",
            "i 1200 loss tensor(0.5980, grad_fn=<CopyBackwards>) train acc: 0.765625 train rec: 0.7351851851851853 train f1 0.7000000000000001\n",
            "time: %.2f 926.9909155368805\n",
            "train loss:  tensor(0.5420, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7479100318471338 rec: 0.7253195365480971 f1: 0.7066301255940489 n: 157\n",
            "{1: 2911, 0: 5785, -1: 1304}\n",
            "i 0 loss tensor(0.4763, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.8226512226512227 train f1 0.8046070460704607\n",
            "i 300 loss tensor(0.4947, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.8120098039215686 train f1 0.8125\n",
            "i 600 loss tensor(0.4740, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.7564102564102564 train f1 0.7332332332332333\n",
            "i 900 loss tensor(0.4835, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.7861111111111111 train f1 0.7733421168203777\n",
            "i 1200 loss tensor(0.3391, grad_fn=<CopyBackwards>) train acc: 0.875 train rec: 0.8623481781376517 train f1 0.8623481781376517\n",
            "time: %.2f 927.6168494224548\n",
            "train loss:  tensor(0.5214, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7609474522292994 rec: 0.7362070607455806 f1: 0.7140477277133015 n: 157\n",
            "{0: 6225, -1: 1482, 1: 2293}\n",
            "i 0 loss tensor(0.4995, grad_fn=<CopyBackwards>) train acc: 0.796875 train rec: 0.6838383838383839 train f1 0.6450216450216449\n",
            "i 300 loss tensor(0.3734, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.8313492063492064 train f1 0.7907137907137907\n",
            "i 600 loss tensor(0.5380, grad_fn=<CopyBackwards>) train acc: 0.71875 train rec: 0.6975524475524475 train f1 0.6860725481415137\n",
            "i 900 loss tensor(0.4260, grad_fn=<CopyBackwards>) train acc: 0.84375 train rec: 0.8381118881118881 train f1 0.8159772326438993\n",
            "i 1200 loss tensor(0.4703, grad_fn=<CopyBackwards>) train acc: 0.828125 train rec: 0.8675213675213675 train f1 0.8174926099808729\n",
            "time: %.2f 928.1296918392181\n",
            "train loss:  tensor(0.4995, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7600517515923567 rec: 0.7298441009682372 f1: 0.722672389425314 n: 157\n",
            "{1: 2696, 0: 5735, -1: 1569}\n",
            "i 0 loss tensor(0.3894, grad_fn=<CopyBackwards>) train acc: 0.875 train rec: 0.8903508771929824 train f1 0.8626373626373627\n",
            "i 300 loss tensor(0.3781, grad_fn=<CopyBackwards>) train acc: 0.921875 train rec: 0.8981481481481483 train f1 0.8989167032645294\n",
            "i 600 loss tensor(0.4666, grad_fn=<CopyBackwards>) train acc: 0.828125 train rec: 0.8231060606060607 train f1 0.8145628543194464\n",
            "i 900 loss tensor(0.4903, grad_fn=<CopyBackwards>) train acc: 0.765625 train rec: 0.7684766214177979 train f1 0.7616376143725372\n",
            "i 1200 loss tensor(0.4322, grad_fn=<CopyBackwards>) train acc: 0.828125 train rec: 0.826388888888889 train f1 0.7759690694278508\n",
            "time: %.2f 927.3852729797363\n",
            "train loss:  tensor(0.4731, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7593550955414012 rec: 0.7287676880727776 f1: 0.7195563105401781 n: 157\n",
            "{0: 5788, -1: 1637, 1: 2575}\n",
            "i 0 loss tensor(0.3838, grad_fn=<CopyBackwards>) train acc: 0.84375 train rec: 0.8661338661338661 train f1 0.8174141096249672\n",
            "i 300 loss tensor(0.5216, grad_fn=<CopyBackwards>) train acc: 0.75 train rec: 0.7309640522875817 train f1 0.7437908496732026\n",
            "i 600 loss tensor(0.4671, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.801295518207283 train f1 0.8002240896358543\n",
            "i 900 loss tensor(0.4215, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.7968013468013467 train f1 0.7675909361955874\n",
            "i 1200 loss tensor(0.4721, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.7551495345612992 train f1 0.7578860550691537\n",
            "time: %.2f 928.0642983913422\n",
            "train loss:  tensor(0.4439, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7508957006369427 rec: 0.7187632532282324 f1: 0.7177264571382489 n: 157\n",
            "{0: 5505, -1: 1664, 1: 2831}\n",
            "i 0 loss tensor(0.3504, grad_fn=<CopyBackwards>) train acc: 0.875 train rec: 0.9098679098679098 train f1 0.8640382317801673\n",
            "i 300 loss tensor(0.3475, grad_fn=<CopyBackwards>) train acc: 0.875 train rec: 0.8772175536881419 train f1 0.8681917211328977\n",
            "i 600 loss tensor(0.4769, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.7205882352941176 train f1 0.7457412838058\n",
            "i 900 loss tensor(0.3189, grad_fn=<CopyBackwards>) train acc: 0.890625 train rec: 0.8904761904761905 train f1 0.8509319777293634\n",
            "i 1200 loss tensor(0.5796, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.7807539682539683 train f1 0.716374269005848\n",
            "time: %.2f 928.828412771225\n",
            "train loss:  tensor(0.4132, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7498009554140127 rec: 0.7118959823243026 f1: 0.7180584992382577 n: 157\n",
            "{0: 5367, -1: 1896, 1: 2737}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHp-9fwN_27V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}