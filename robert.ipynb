{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tguMQAVh-Tpr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "2f67dda9-847e-4f12-a5ef-c73902288c56"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 21 15:57:36 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B0YAdiF-mrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "os.chdir('drive/My Drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK0FHtWE-rzR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "02e4d508-f830-433e-a4cb-6c6903ad9510"
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 6.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 6.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.2 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.2->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.2->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.2->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OusbxFvo-uda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from tqdm import tqdm\n",
        "import torch.utils.data.dataloader as dataloader\n",
        "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
        "from pytorch_pretrained_bert import BertForSequenceClassification,BertModel\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import time\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wstSB2NU-yjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 140\n",
        "batch_size = 32\n",
        "epochs = 9\n",
        "input_categories = '微博中文内容'\n",
        "output_categories = '情感倾向'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kRFXC5d_5WB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('bert_ipy/nCoV_100k_train.labled.csv',header=0)\n",
        "test_df = pd.read_csv('bert_ipy/nCov_10k_test.csv',header=0)\n",
        "train_df1 = train_df.copy()\n",
        "train_df1=train_df1[~train_df1['情感倾向'].isin(['9','-','·'])]\n",
        "train_df1 = train_df1.fillna(10)\n",
        "train_df1=train_df1[~train_df1['情感倾向'].isin(['-2','10','4',10])]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQEdPnf9_42o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "06b2f817-11fd-47a1-9046-4bfe38bb1988"
      },
      "source": [
        "import random\n",
        "\n",
        "k_fold=[]\n",
        "k_fold_2 = []\n",
        "index=set(range(train_df1.shape[0]))\n",
        "\n",
        "tmp=random.sample(list(index),10000)\n",
        "k_fold.append(tmp)\n",
        "index-=set(tmp)\n",
        "k_fold_2.append(list(index))\n",
        "\n",
        "train_df1.iloc[k_fold_2[0]].to_csv(\"data/train_6\",sep=\",\",index=False)\n",
        "train_df1.iloc[k_fold[0]].to_csv(\"data/val_6\",sep=\",\",index=False)\n",
        "print(\"done!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-53f6cbfec3a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mk_fold_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_df1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_fold_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/train_6\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtrain_df1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/val_6\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train_6'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWpL4noEo3a4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "84b937c9-5b42-494a-b8f7-1ea1b44d0942"
      },
      "source": [
        "import random\n",
        "def k_fold_split(train,k):\n",
        "    os.system(\"mkdir data\")\n",
        "    k_fold=[]\n",
        "    index=set(range(train.shape[0]))\n",
        "    for i in range(k):\n",
        "        #防止所有数据不能整除k，最后将剩余的都放到最后一折\n",
        "        if i==k-1:\n",
        "            k_fold.append(list(index))\n",
        "        else:\n",
        "            tmp=random.sample(list(index),int(1.0/k*train.shape[0]))\n",
        "            k_fold.append(tmp)\n",
        "            index-=set(tmp)\n",
        "    #将原始训练集划分为k个包含训练集和验证集的训练集，同时每个训练集中，训练集：验证集=k-1:1\n",
        "    for i in range(k):\n",
        "        print(\"第{}折........\".format(i+1))\n",
        "        tra=[]\n",
        "        dev=k_fold[i]\n",
        "        for j in range(k):\n",
        "            if i!=j:\n",
        "                tra+=k_fold[j]\n",
        "        train.iloc[tra].to_csv(\"data/train_{}\".format(i),sep=\",\",index=False)\n",
        "        train.iloc[dev].to_csv(\"data/val_{}\".format(i),sep=\",\",index=False)\n",
        "    print(\"done!\")\n",
        " \n",
        " \n",
        "k_fold_split(train_df1,5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "第1折........\n",
            "第2折........\n",
            "第3折........\n",
            "第4折........\n",
            "第5折........\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNR-JpZ3-4eb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5765a868-672a-4d46-e062-5314fa12a31c"
      },
      "source": [
        "train_0 = pd.read_csv(\"data/train_1\")\n",
        "val_0 = pd.read_csv(\"data/val_1\")\n",
        "print(train_0.shape,val_0.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(79931, 7) (19982, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0xI-4SU-7e6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_0 = train_0[:1024]\n",
        "val_0 = val_0[:128]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnawHdhNBNQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _convert_to_transformer_inputs(instance,tokenizer,max_sequence_length):\n",
        "    def return_id(str1,truncation_strategy,length):\n",
        "        inputs = tokenizer.tokenize(str1)\n",
        "        if len(inputs) > 138:\n",
        "          inputs = inputs[:138]\n",
        "        inputs = [\"[CLS]\"]+ inputs + [\"[SEP]\"]\n",
        "        input_ids =  tokenizer.convert_tokens_to_ids(inputs)\n",
        "#         print(input_ids)\n",
        "        input_masks = [1] * len(input_ids)\n",
        "#         print(input_masks)\n",
        "        input_segments = [0] * len(input_ids)\n",
        "        padding_length = length - len(input_ids)\n",
        "#         padding_id = tokenizer.pad_token_id\n",
        "        input_ids = input_ids + ([0] * padding_length)\n",
        "        input_masks = input_masks + ([0] * padding_length)\n",
        "        input_segments = input_segments + ([0] * padding_length)\n",
        "        # if len(input_ids) != 200:\n",
        "        #   print(str1,len(input_ids))\n",
        "        return [input_ids, input_masks, input_segments]\n",
        "    \n",
        "    input_ids,input_masks,input_segments = return_id(instance, 'longest_first', max_sequence_length)\n",
        "    return [input_ids, input_masks,input_segments]\n",
        "\n",
        "\n",
        "def compute_input_arrays(df,columns,tokenizer,max_sequence_length):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for instance in tqdm(df[columns]):\n",
        "        \n",
        "        ids, masks, segments = \\\n",
        "        _convert_to_transformer_inputs(str(instance), tokenizer, max_sequence_length)\n",
        "        \n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "    # print(input_ids)\n",
        "\n",
        "    return input_ids, input_masks, input_segments\n",
        "\n",
        "def compute_output_arrays(df,columns):\n",
        "    return np.asarray(df[columns].astype(int) + 1)\n",
        "\n",
        "def data_loader(input_ids,input_masks,input_segments,label_ids):\n",
        "    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "    all_input_mask = torch.tensor(input_masks, dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor(input_segments, dtype=torch.long)\n",
        "    all_label = torch.tensor(label_ids, dtype=torch.long)\n",
        "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n",
        "    return train_dataloader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iem2TrcXgyVF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bed2fe8d-0124-44c9-b0b3-6c32fe721068"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "f = zipfile.ZipFile(\"chinese_roberta_wwm_ext_pytorch.zip\",'r')\n",
        "for file in f.namelist():\n",
        "  f.extract(file,'roberta/chinese_roberta_wwm_ext_pytorch/')\n",
        "f.close\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method ZipFile.close of <zipfile.ZipFile filename='chinese_roberta_wwm_ext_pytorch.zip' mode='r'>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJQhmPuVBSqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    # self.bert = BertModel.from_pretrained(\"chinese_roberta_wwm_ext_pytorch/\")\n",
        "    self.bert = BertModel.from_pretrained(pretrained_model_name_or_path='roberta/chinese_roberta_wwm_ext_pytorch/')\n",
        "\n",
        "    for param in self.bert.parameters():\n",
        "      param.requires_grad = True \n",
        "    self.fc = nn.Linear(768,3)\n",
        "  \n",
        "  def forward(self,input_ids,input_mask,segment_ids):\n",
        "    _, pooled = self.bert(input_ids,token_type_ids = segment_ids,attention_mask = input_mask,output_all_encoded_layers= False)\n",
        "    out = self.fc(pooled)\n",
        "    return out \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb6tfy2OBWAK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d35687ec-49aa-4acd-e80f-83c14a58999f"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('roberta/chinese_roberta_wwm_ext_pytorch/vocab.txt')\n",
        "t_input_ids, t_input_masks, t_input_segments = compute_input_arrays(train_0,input_categories,tokenizer,MAX_SEQUENCE_LENGTH)\n",
        "print(len(t_input_segments))\n",
        "t_label_ids = compute_output_arrays(train_0, output_categories)\n",
        "train_dataloader = data_loader(t_input_ids, t_input_masks, t_input_segments,t_label_ids)\n",
        "\n",
        "v_input_ids, v_input_masks, v_input_segments = compute_input_arrays(val_0,input_categories,tokenizer,MAX_SEQUENCE_LENGTH)\n",
        "v_label_ids = compute_output_arrays(val_0, output_categories)\n",
        "val_dataloader = data_loader(v_input_ids, v_input_masks, v_input_segments,v_label_ids)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "# bert_model = BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path='bert-base-chinese', num_labels=3)\n",
        "bert_model = Model().to(device)\n",
        "param_optimizer = list(bert_model.named_parameters())  # 模型参数名字列表\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "NUM_EPOCHS =8\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=0.05,\n",
        "                     t_total=len(train_0) * NUM_EPOCHS)\n",
        "# optimizer = BertAdam(bert_model.parameters(), lr=1e-6)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 79931/79931 [00:49<00:00, 1617.90it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "79931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19982/19982 [00:12<00:00, 1660.38it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJF-Vzrh6Tmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b4d53fa-e33f-45f0-c482-2714716fb961"
      },
      "source": [
        "test_df = pd.read_csv('bert_ipy/nCov_10k_test.csv',header=0)\n",
        "# test_df = pd.read_csv('nCov_10k_test.csv',header=0)\n",
        "test_df1 = test_df.copy()\n",
        "# test_df1.info()\n",
        "dev_input_ids, dev_input_masks, dev_input_segments = compute_input_arrays(test_df1,input_categories,tokenizer,MAX_SEQUENCE_LENGTH)\n",
        "def test_loader(input_ids,input_masks,input_segments):\n",
        "    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "    all_input_mask = torch.tensor(input_masks, dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor(input_segments, dtype=torch.long)\n",
        "    \n",
        "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    return train_dataloader\n",
        "\n",
        "dev_dataloader = test_loader(dev_input_ids, dev_input_masks, dev_input_segments)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:05<00:00, 1671.38it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJAyBA-BZVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.metrics import f1_score\n",
        "class OptimizedF1(object):\n",
        "    def __init__(self):\n",
        "        self.coef_ = []\n",
        "\n",
        "    def _kappa_loss(self, coef, X, y):\n",
        "        \"\"\"\n",
        "        y_hat = argmax(coef*X, axis=-1)\n",
        "        :param coef: (1D array) weights\n",
        "        :param X: (2D array)logits\n",
        "        :param y: (1D array) label\n",
        "        :return: -f1\n",
        "        \"\"\"\n",
        "        # print(\"X:\",X)\n",
        "        X_p = np.copy(X)\n",
        "        # print(\"X_p:\",type(X_p),\"coef:\",coef)\n",
        "        X_p = X_p*coef\n",
        "        ll = f1_score(y, np.argmax(X_p, axis=-1), average='macro')\n",
        "        return -ll\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
        "        initial_coef = [1. for _ in range(3)]\n",
        "        # print(\"initial_coef :\",initial_coef)\n",
        "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
        "\n",
        "    def predict(self, X, y):\n",
        "        X_p = np.copy(X)\n",
        "        X_p = self.coef_['x'] * X_p\n",
        "        print(\"X_p:\",X_p,\"f1:\",f1_score(y, np.argmax(X_p, axis=-1), average='macro'))\n",
        "        return f1_score(y, np.argmax(X_p, axis=-1), average='macro')\n",
        "\n",
        "    def coefficients(self):\n",
        "        return self.coef_['x']\n",
        "\n",
        "    def prt_coef(self):\n",
        "        am= self.coef_\n",
        "        print(am)\n",
        "op = OptimizedF1()\n",
        "# op.fit(logits,labels)\n",
        "# logits = op.coefficients()*logits "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5KE8EVNB068",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "from torch.autograd import Variable\n",
        "\n",
        "def change_csv(abblist):\n",
        "    abclist=[]\n",
        "    for i in range(157):\n",
        "      if i != 156 :\n",
        "        for j in range(64):\n",
        "            abclist.append(int(abblist[i][j])-1)\n",
        "      else:\n",
        "        for j in range(16):\n",
        "          abclist.append(int(abblist[i][j])-1)\n",
        "    dic1={}\n",
        "    for i in abclist:\n",
        "        dic1[i] = abclist.count(i)\n",
        "    print(dic1)\n",
        "    return abclist\n",
        "\n",
        "def create_test_csv(abclist):\n",
        "    df1_test = test_df1.copy()\n",
        "    df1_test['id']=df1_test[\"微博id\"]\n",
        "    df2_test_pud=df1_test.drop(labels=['微博id','微博发布时间',\"发布人账号\",'微博中文内容','微博图片','微博视频'],axis=1)\n",
        "    letters_test_pud = ['id']\n",
        "    df7_test=df2_test_pud[letters_test_pud]\n",
        "    df7_test['y']=None\n",
        "    df7_test['y'] = abclist\n",
        "    # df_sub['id'] = df_sub['id'].apply(lambda x: str(x)+' ')\n",
        "    # df7_test.to_csv('test_03341.csv',index=False, encoding='utf-8')\n",
        "\n",
        "    return df7_test\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    i = 0\n",
        "    \n",
        "    for input_ids, segment_ids,input_mask,label_ids in iterator:\n",
        "        input_ids = input_ids.to(device)\n",
        "        input_mask = input_mask.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        label_ids = label_ids.to(device)\n",
        "        logits = model(input_ids,segment_ids,input_mask)\n",
        "        logits2 = logits.cpu().detach()\n",
        "        y_pred_notrick=logits.argmax(dim=1).cpu()\n",
        "        # print(\"logits1:\",logits.argmax(dim=1))\n",
        "        model.zero_grad()\n",
        "        # if i %600 == 0:\n",
        "        #     print(\"---未测试时coef的值---------------\")\n",
        "        #     op.prt_coef()\n",
        "        op.fit(logits2,label_ids.cpu())\n",
        "        tips = Variable(torch.Tensor(op.coefficients()),requires_grad=True)\n",
        "        # if i %600 == 0:\n",
        "        #     print(\"---测试后coef的值---------------\")\n",
        "        #     op.prt_coef()\n",
        "        logits = tips*(logits.cpu())\n",
        "        y_pred_c = logits.argmax(dim=1).cpu()\n",
        "        logits = logits.to(device)\n",
        "        # print(\"logits2:\",logits.argmax(dim=1)) \n",
        "        # y_pred_label = y_pred.cpu()\n",
        "        loss = F.cross_entropy(logits, label_ids)\n",
        "        epoch_loss += loss.cpu()\n",
        "        # y_pred_c = y_pred.argmax(dim=1).cpu()\n",
        "        # print(y_pred_c)\n",
        "        label_ids_c = label_ids.cpu()\n",
        "        # print(label_ids_c)\n",
        "        if i %300== 0:\n",
        "          # print(\"pred_notrick:\",y_pred_notrick)\n",
        "          # print(\"pred_trick:\",y_pred_c)\n",
        "          # print(\"label:\",label_ids_c)\n",
        "          # op.prt_coef()\n",
        "          print(\"i\",i,\"loss\",loss.cpu(),\"train acc:\",accuracy_score(y_pred_c,label_ids_c),\"train rec:\",recall_score(y_pred_c,label_ids_c,average='macro'),\"train f1\",f1_score(y_pred_c,label_ids_c,average='macro'))\n",
        "        loss.backward()\n",
        "        optimizer.step() \n",
        "        i += 1\n",
        "    end = time.time()\n",
        "    runtime = end-start\n",
        "    print('time: %.2f' , runtime)\n",
        "    return epoch_loss / len(iterator)\n",
        "def deval(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    abblist = []\n",
        "    n = 0\n",
        "    f1 = 0\n",
        "    acc = 0\n",
        "    rec = 0\n",
        "    with torch.no_grad():\n",
        "      # print(\"----------pred time the coef:----------\")\n",
        "      # op.prt_coef()\n",
        "      for input_ids,  segment_ids,input_mask,label_ids in iterator:\n",
        "        \n",
        "        n += 1\n",
        "        input_ids = input_ids.to(device)\n",
        "        input_mask = input_mask.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        output = model(input_ids,segment_ids,input_mask)\n",
        "\n",
        "        # logits2 = output.cpu().detach()\n",
        "        \n",
        "        # tips2 = Variable(torch.Tensor(op.coefficients()),requires_grad=True)\n",
        "        logits = output.cpu()\n",
        "        # print(\"-------------test-------------------\")\n",
        "        # print(\"label:\",output.argmax(dim=1).cpu())\n",
        "        # print(\"pred:\",logits.argmax(dim=1).cpu())\n",
        "        # print(\"-------------test-------------------\")\n",
        "        # op.prt_coef()\n",
        "        y_pred_label = logits.argmax(dim=1).cpu()\n",
        "        acc += accuracy_score(y_pred_label,label_ids)\n",
        "        rec += recall_score(y_pred_label,label_ids,average='macro')\n",
        "        f1 += f1_score(y_pred_label,label_ids,average='macro')\n",
        "      print(\"train acc :\",acc/n, \"rec:\",rec/n,\"f1:\",f1/n,'n:',n)\n",
        "def pred(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    abblist = []\n",
        "    abclist = []\n",
        "    with torch.no_grad():\n",
        "      for input_ids,  segment_ids,input_mask in iterator:\n",
        "          input_ids = input_ids.to(device)\n",
        "          input_mask = input_mask.to(device)\n",
        "          segment_ids = segment_ids.to(device)\n",
        "          output = model(input_ids,segment_ids,input_mask)\n",
        "          # tips2 = Variable(torch.Tensor(op.coefficients()),requires_grad=True)\n",
        "          # logits = tips2*(output.cpu())\n",
        "          y_pred_label = output.argmax(dim=1).cpu()\n",
        "          abblist.append(y_pred_label)\n",
        "      abclist = change_csv(abblist)\n",
        "      test_csv = create_test_csv(abclist)\n",
        "    return test_csv\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIrYuqcaE73q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d496255-f9f8-4759-adc8-dd0bf255d6b9"
      },
      "source": [
        "stat = 0 \n",
        "for i in range(epochs):\n",
        "    train_loss = train(bert_model, train_dataloader, optimizer, criterion, device) \n",
        "    lss = \"roberta_model/p100_roberta_trick_\"+str(i)+\"_.pk1\"\n",
        "    if i == 0 :\n",
        "      stat = train_loss\n",
        "    if i != 0 :\n",
        "      if stat-train_loss < 0.005 :\n",
        "        break\n",
        "      if stat-train_loss < 0 :\n",
        "        break\n",
        "    torch.save(bert_model.state_dict(), lss)\n",
        "    print(\"train loss: \", train_loss)\n",
        "    deval(bert_model, val_dataloader, criterion, device)\n",
        "    test_csv = pred(bert_model, dev_dataloader, criterion, device)\n",
        "    test_csv.to_csv('roberta_model/test_bwetick_'+str(i)+'.csv',index=False, encoding='utf-8')\n",
        "    # deval(bert_model, train_dataloader, criterion, device)\n",
        "torch.save(bert_model.state_dict(), \"roberta_model/p100_roberta_trick_true_end.pk1\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i 0 loss tensor(1.1254, grad_fn=<CopyBackwards>) train acc: 0.28125 train rec: 0.3680272108843537 train f1 0.24695198592257417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "i 300 loss tensor(0.9739, grad_fn=<CopyBackwards>) train acc: 0.53125 train rec: 0.3440860215053763 train f1 0.2662037037037037\n",
            "i 600 loss tensor(0.8676, grad_fn=<CopyBackwards>) train acc: 0.609375 train rec: 0.532258064516129 train f1 0.3193336877547404\n",
            "i 900 loss tensor(0.6870, grad_fn=<CopyBackwards>) train acc: 0.703125 train rec: 0.6223290598290597 train f1 0.5447330447330447\n",
            "i 1200 loss tensor(0.6855, grad_fn=<CopyBackwards>) train acc: 0.6875 train rec: 0.7451690821256038 train f1 0.6458333333333334\n",
            "time: %.2f 1378.6436052322388\n",
            "train loss:  tensor(0.8193, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7230294585987261 rec: 0.6930315489910012 f1: 0.6750532954896564 n: 157\n",
            "{1: 2312, 0: 6176, -1: 1512}\n",
            "i 0 loss tensor(0.6020, grad_fn=<CopyBackwards>) train acc: 0.71875 train rec: 0.6291666666666667 train f1 0.6396188478013811\n",
            "i 300 loss tensor(0.6995, grad_fn=<CopyBackwards>) train acc: 0.75 train rec: 0.7786324786324786 train f1 0.6851387728580711\n",
            "i 600 loss tensor(0.5743, grad_fn=<CopyBackwards>) train acc: 0.765625 train rec: 0.6824074074074074 train f1 0.6667163067758749\n",
            "i 900 loss tensor(0.6074, grad_fn=<CopyBackwards>) train acc: 0.75 train rec: 0.796425345205833 train f1 0.7262824321647852\n",
            "i 1200 loss tensor(0.6079, grad_fn=<CopyBackwards>) train acc: 0.765625 train rec: 0.7727898176814586 train f1 0.7343738387216648\n",
            "time: %.2f 1378.0725719928741\n",
            "train loss:  tensor(0.5963, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7406449044585988 rec: 0.70922546620215 f1: 0.6989014105754128 n: 157\n",
            "{1: 2457, 0: 5952, -1: 1591}\n",
            "i 0 loss tensor(0.5812, grad_fn=<CopyBackwards>) train acc: 0.765625 train rec: 0.784449221949222 train f1 0.7522435897435898\n",
            "i 300 loss tensor(0.4961, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.7502963489805595 train f1 0.7344054580896686\n",
            "i 600 loss tensor(0.5144, grad_fn=<CopyBackwards>) train acc: 0.75 train rec: 0.7473262032085563 train f1 0.7292337627482555\n",
            "i 900 loss tensor(0.4649, grad_fn=<CopyBackwards>) train acc: 0.90625 train rec: 0.846031746031746 train f1 0.8681945907973305\n",
            "i 1200 loss tensor(0.4825, grad_fn=<CopyBackwards>) train acc: 0.828125 train rec: 0.8243525480367585 train f1 0.8068068068068069\n",
            "time: %.2f 1380.7131555080414\n",
            "train loss:  tensor(0.5624, grad_fn=<DivBackward0>)\n",
            "train acc : 0.745421974522293 rec: 0.7222180049549871 f1: 0.6974992421761391 n: 157\n",
            "{1: 2611, 0: 6052, -1: 1337}\n",
            "i 0 loss tensor(0.4402, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.7851303735024665 train f1 0.7705627705627706\n",
            "i 300 loss tensor(0.5325, grad_fn=<CopyBackwards>) train acc: 0.765625 train rec: 0.7793417366946779 train f1 0.7248803827751197\n",
            "i 600 loss tensor(0.4926, grad_fn=<CopyBackwards>) train acc: 0.796875 train rec: 0.8015873015873017 train f1 0.766954457506138\n",
            "i 900 loss tensor(0.4896, grad_fn=<CopyBackwards>) train acc: 0.84375 train rec: 0.8253253253253253 train f1 0.8157392686804452\n",
            "i 1200 loss tensor(0.5402, grad_fn=<CopyBackwards>) train acc: 0.765625 train rec: 0.7233193277310924 train f1 0.7019598545914335\n",
            "time: %.2f 1380.5341503620148\n",
            "train loss:  tensor(0.5392, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7534832802547771 rec: 0.7306385395297879 f1: 0.7091466095518206 n: 157\n",
            "{1: 2394, 0: 6175, -1: 1431}\n",
            "i 0 loss tensor(0.4025, grad_fn=<CopyBackwards>) train acc: 0.796875 train rec: 0.7195767195767195 train f1 0.7290886392009988\n",
            "i 300 loss tensor(0.5721, grad_fn=<CopyBackwards>) train acc: 0.796875 train rec: 0.7884395424836601 train f1 0.781973581973582\n",
            "i 600 loss tensor(0.5827, grad_fn=<CopyBackwards>) train acc: 0.71875 train rec: 0.6925064599483203 train f1 0.6589788812011035\n",
            "i 900 loss tensor(0.3370, grad_fn=<CopyBackwards>) train acc: 0.859375 train rec: 0.8631652661064425 train f1 0.8388167388167388\n",
            "i 1200 loss tensor(0.6115, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.7327734711455642 train f1 0.7275823532542174\n",
            "time: %.2f 1380.1160774230957\n",
            "train loss:  tensor(0.5155, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7546775477707006 rec: 0.7227195024486268 f1: 0.7192619971544535 n: 157\n",
            "{1: 2471, 0: 5805, -1: 1724}\n",
            "i 0 loss tensor(0.5542, grad_fn=<CopyBackwards>) train acc: 0.765625 train rec: 0.7649048625792813 train f1 0.7154932757574475\n",
            "i 300 loss tensor(0.5688, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.8354497354497354 train f1 0.7419005157793513\n",
            "i 600 loss tensor(0.5104, grad_fn=<CopyBackwards>) train acc: 0.84375 train rec: 0.8591591591591592 train f1 0.8290936117023073\n",
            "i 900 loss tensor(0.4489, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.8266769073220686 train f1 0.8086568603809984\n",
            "i 1200 loss tensor(0.4479, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.7644777986241401 train f1 0.7715665976535542\n",
            "time: %.2f 1380.7582368850708\n",
            "train loss:  tensor(0.4910, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7508957006369427 rec: 0.7142906646629783 f1: 0.717613878903947 n: 157\n",
            "{1: 2663, 0: 5533, -1: 1804}\n",
            "i 0 loss tensor(0.5933, grad_fn=<CopyBackwards>) train acc: 0.765625 train rec: 0.7426282051282053 train f1 0.7426262626262626\n",
            "i 300 loss tensor(0.4353, grad_fn=<CopyBackwards>) train acc: 0.84375 train rec: 0.827524893314367 train f1 0.8129294452823865\n",
            "i 600 loss tensor(0.3919, grad_fn=<CopyBackwards>) train acc: 0.890625 train rec: 0.8854636591478696 train f1 0.8854636591478696\n",
            "i 900 loss tensor(0.6213, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.8607723577235772 train f1 0.7733918128654972\n",
            "i 1200 loss tensor(0.5279, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.8090277777777778 train f1 0.8136629452418926\n",
            "time: %.2f 1377.1335327625275\n",
            "train loss:  tensor(0.4604, grad_fn=<DivBackward0>)\n",
            "train acc : 0.7540804140127388 rec: 0.7274690708809379 f1: 0.7135493057660711 n: 157\n",
            "{1: 2588, 0: 5896, -1: 1516}\n",
            "i 0 loss tensor(0.4199, grad_fn=<CopyBackwards>) train acc: 0.859375 train rec: 0.8898809523809524 train f1 0.8446368446368447\n",
            "i 300 loss tensor(0.6325, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.7666666666666666 train f1 0.7450234741784038\n",
            "i 600 loss tensor(0.2437, grad_fn=<CopyBackwards>) train acc: 0.9375 train rec: 0.9486461251167134 train f1 0.9303921568627452\n",
            "i 900 loss tensor(0.3488, grad_fn=<CopyBackwards>) train acc: 0.875 train rec: 0.8363408521303258 train f1 0.8538011695906432\n",
            "i 1200 loss tensor(0.3723, grad_fn=<CopyBackwards>) train acc: 0.859375 train rec: 0.835923335923336 train f1 0.8552464788732395\n",
            "time: %.2f 1375.062798500061\n",
            "train loss:  tensor(0.4235, grad_fn=<DivBackward0>)\n",
            "train acc : 0.755672770700637 rec: 0.7232384864708515 f1: 0.7170911327050971 n: 157\n",
            "{0: 5852, -1: 1697, 1: 2451}\n",
            "i 0 loss tensor(0.4609, grad_fn=<CopyBackwards>) train acc: 0.859375 train rec: 0.8856209150326797 train f1 0.8480664790944187\n",
            "i 300 loss tensor(0.3619, grad_fn=<CopyBackwards>) train acc: 0.921875 train rec: 0.956140350877193 train f1 0.9184450135154361\n",
            "i 600 loss tensor(0.4377, grad_fn=<CopyBackwards>) train acc: 0.75 train rec: 0.7668650793650794 train f1 0.6681805745554037\n",
            "i 900 loss tensor(0.5552, grad_fn=<CopyBackwards>) train acc: 0.765625 train rec: 0.809368191721133 train f1 0.7656055235803554\n",
            "i 1200 loss tensor(0.3596, grad_fn=<CopyBackwards>) train acc: 0.859375 train rec: 0.8575276575276575 train f1 0.8475256769374416\n",
            "time: %.2f 1375.9148306846619\n",
            "train loss:  tensor(0.3848, grad_fn=<DivBackward0>)\n",
            "train acc : 0.748109076433121 rec: 0.7179847462392996 f1: 0.710751394831098 n: 157\n",
            "{1: 3001, 0: 5548, -1: 1451}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ycQc7A-FMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcBEA5qD-FA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24f3i0eO-HMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrPtYSId-HJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Du_9C--HG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxz3KHsW-HD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37n7tV1J-G_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ3PVaE3-G4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aafw_ormFEtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv('nCov_10k_test.csv',header=0)\n",
        "# test_df = pd.read_csv('nCov_10k_test.csv',header=0)\n",
        "test_df1 = test_df.copy()\n",
        "# test_df1.info()\n",
        "dev_input_ids, dev_input_masks, dev_input_segments = compute_input_arrays(test_df1,input_categories,tokenizer,MAX_SEQUENCE_LENGTH)\n",
        "def test_loader(input_ids,input_masks,input_segments):\n",
        "    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "    all_input_mask = torch.tensor(input_masks, dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor(input_segments, dtype=torch.long)\n",
        "    \n",
        "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    return train_dataloader\n",
        "\n",
        "dev_dataloader = test_loader(dev_input_ids, dev_input_masks, dev_input_segments)\n",
        "\n",
        "state_dict = torch.load(\"roberta/p100_9_.pk1\")\n",
        "bert_model.load_state_dict(state_dict)\n",
        "def pred(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    abblist = []\n",
        "    with torch.no_grad():\n",
        "      for input_ids,  segment_ids,input_mask in iterator:\n",
        "          input_ids = input_ids.to(device)\n",
        "          input_mask = input_mask.to(device)\n",
        "          segment_ids = segment_ids.to(device)\n",
        "          output = model(input_ids,segment_ids,input_mask)\n",
        "          y_pred_label = output.argmax(dim=1).cpu()\n",
        "          abblist.append(y_pred_label)\n",
        "    return abblist\n",
        "\n",
        "abblist = pred(bert_model, dev_dataloader, criterion, device)\n",
        "abclist=[]\n",
        "for i in range(157):\n",
        "  if i != 156 :\n",
        "    for j in range(64):\n",
        "        abclist.append(int(abblist[i][j])-1)\n",
        "  else:\n",
        "    for j in range(16):\n",
        "      abclist.append(int(abblist[i][j])-1)\n",
        "dic1={}\n",
        "for i in abclist:\n",
        "    dic1[i] = abclist.count(i)\n",
        "dic1\n",
        "\n",
        "df1_test = test_df1.copy()\n",
        "df1_test['id']=df1_test[\"微博id\"]\n",
        "df2_test_pud=df1_test.drop(labels=['微博id','微博发布时间',\"发布人账号\",'微博中文内容','微博图片','微博视频'],axis=1)\n",
        "letters_test_pud = ['id']\n",
        "df7_test=df2_test_pud[letters_test_pud]\n",
        "df7_test['y']=None\n",
        "df7_test['y'] = abclist\n",
        "# df_sub['id'] = df_sub['id'].apply(lambda x: str(x)+' ')\n",
        "df7_test.to_csv('test_03341.csv',index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}