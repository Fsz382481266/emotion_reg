{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBqS8Chu0mY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "03eabd4e-c0c5-4253-818c-d8bc09030d92"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Mar 25 05:44:41 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZmjNLny00z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "os.chdir('drive/My Drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNrRKgIy8IyO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62fb7843-a682-4e69-f7b3-d9750313e0e4"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "f = zipfile.ZipFile(\"xlnet/chinese_xlnet_base_pytorch.zip\",'r')\n",
        "for file in f.namelist():\n",
        "  f.extract(file,'xlnet/')\n",
        "f.close"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method ZipFile.close of <zipfile.ZipFile filename='xlnet/chinese_xlnet_base_pytorch.zip' mode='r'>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bwR4KrF0-yH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "e2268261-101e-4a6c-fb3d-fe8e9ef6cc16"
      },
      "source": [
        "!pip install pytorch_transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 30.2MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 33.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 39.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 44.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 25.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 27.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 21.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 20.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 20.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102kB 20.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 20.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 20.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133kB 20.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 20.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 20.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163kB 20.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174kB 20.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 20.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.18.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.12.26)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.38.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\r\u001b[K     |▍                               | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 30.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 37.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 41.1MB/s eta 0:00:01\r\u001b[K     |██                              | 51kB 42.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 41.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 40.4MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 39.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 40.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 102kB 36.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 112kB 36.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 122kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 133kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 143kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 153kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 174kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 184kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 194kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 204kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 215kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 225kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 235kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 245kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 256kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 266kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 276kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 286kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 296kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 307kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 317kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 327kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 337kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 348kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 358kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 368kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 378kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 389kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 399kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 409kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 419kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 430kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 440kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 450kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 460kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 471kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 481kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 491kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 501kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 512kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 522kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 532kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 542kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 552kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 563kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 573kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 583kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 593kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 604kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 614kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 624kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 634kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 645kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 655kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 665kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 675kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 686kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 696kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 706kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 716kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 727kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 737kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 747kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 757kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 768kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 778kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 788kB 36.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 798kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 808kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 819kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 829kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 839kB 36.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 849kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 860kB 36.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 870kB 36.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 24.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 35.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 45.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 48.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 51.5MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 55.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 56.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 57.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 59.2MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 61.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 61.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 61.6MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 61.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 61.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 61.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 61.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 61.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 61.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 61.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 61.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 61.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 61.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 61.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 61.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 61.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 61.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 61.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 61.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 61.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 40kB/s eta 0:00:09\r\u001b[K     |██████████████████████▍         | 727kB 40kB/s eta 0:00:08\r\u001b[K     |██████████████████████▊         | 737kB 40kB/s eta 0:00:08\r\u001b[K     |███████████████████████         | 747kB 40kB/s eta 0:00:08\r\u001b[K     |███████████████████████▎        | 757kB 40kB/s eta 0:00:08\r\u001b[K     |███████████████████████▋        | 768kB 40kB/s eta 0:00:07\r\u001b[K     |████████████████████████        | 778kB 40kB/s eta 0:00:07\r\u001b[K     |████████████████████████▎       | 788kB 40kB/s eta 0:00:07\r\u001b[K     |████████████████████████▋       | 798kB 40kB/s eta 0:00:06\r\u001b[K     |█████████████████████████       | 808kB 40kB/s eta 0:00:06\r\u001b[K     |█████████████████████████▏      | 819kB 40kB/s eta 0:00:06\r\u001b[K     |█████████████████████████▌      | 829kB 40kB/s eta 0:00:06\r\u001b[K     |█████████████████████████▉      | 839kB 40kB/s eta 0:00:05\r\u001b[K     |██████████████████████████▏     | 849kB 40kB/s eta 0:00:05\r\u001b[K     |██████████████████████████▌     | 860kB 40kB/s eta 0:00:05\r\u001b[K     |██████████████████████████▉     | 870kB 40kB/s eta 0:00:05\r\u001b[K     |███████████████████████████     | 880kB 40kB/s eta 0:00:04\r\u001b[K     |███████████████████████████▍    | 890kB 40kB/s eta 0:00:04\r\u001b[K     |███████████████████████████▊    | 901kB 40kB/s eta 0:00:04\r\u001b[K     |████████████████████████████    | 911kB 40kB/s eta 0:00:04\r\u001b[K     |████████████████████████████▍   | 921kB 40kB/s eta 0:00:03\r\u001b[K     |████████████████████████████▊   | 931kB 40kB/s eta 0:00:03\r\u001b[K     |█████████████████████████████   | 942kB 40kB/s eta 0:00:03\r\u001b[K     |█████████████████████████████▎  | 952kB 40kB/s eta 0:00:03\r\u001b[K     |█████████████████████████████▋  | 962kB 40kB/s eta 0:00:02\r\u001b[K     |██████████████████████████████  | 972kB 40kB/s eta 0:00:02\r\u001b[K     |██████████████████████████████▎ | 983kB 40kB/s eta 0:00:02\r\u001b[K     |██████████████████████████████▋ | 993kB 40kB/s eta 0:00:02\r\u001b[K     |██████████████████████████████▉ | 1.0MB 40kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 40kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 40kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 40kB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.26 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.15.26)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->pytorch_transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->pytorch_transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=65cbd9d38d0af46e386db3ddea628ab131c7926631ee81eda313d1fba9cd52c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.38 sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgQaNGx01Pl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers.modeling_xlnet import XLNetForSequenceClassification, XLNetConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
        "from pytorch_transformers.tokenization_xlnet import XLNetTokenizer\n",
        "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
        "from tqdm import tqdm\n",
        "import torch.utils.data.dataloader as dataloader\n",
        "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
        "# from pytorch_pretrained_bert import BertForSequenceClassification,BertModel\n",
        "# from pytorch_transformers import Adamw\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPFGzbyh1awl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 140\n",
        "batch_size = 64\n",
        "epochs = 1500\n",
        "input_categories = '微博中文内容'\n",
        "output_categories = '情感倾向'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgvYuwpw2Ihy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_0 = pd.read_csv(\"data/train_1\")\n",
        "val_0 = pd.read_csv(\"data/val_1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUak6U4U2j7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_0 = train_0[:128]\n",
        "val_0 = val_0[:128]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSjYORw42NOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _convert_to_transformer_inputs(instance,tokenizer,max_sequence_length):\n",
        "    def return_id(str1,truncation_strategy,length):\n",
        "        inputs = tokenizer.tokenize(str1)\n",
        "        if len(inputs) > 137:\n",
        "          inputs = inputs[:137]\n",
        "        inputs = [\"<sep>\"]+ inputs + [\"<sep>\"]+[\"<cls>\"]\n",
        "        input_ids =  tokenizer.convert_tokens_to_ids(inputs)\n",
        "        # print(len(input_ids))\n",
        "        input_masks = [1] * len(input_ids)\n",
        "        # print(len(input_masks))\n",
        "        input_segments = [1] * (len(inputs)-1)+[2]\n",
        "        # print(len(input_segments))\n",
        "        padding_length = length - len(input_ids)\n",
        "#         padding_id = tokenizer.pad_token_id\n",
        "        input_ids = ([0] * padding_length) + input_ids \n",
        "        input_masks = ([0] * padding_length) + input_masks \n",
        "        input_segments = ([4] * padding_length) + input_segments \n",
        "        # if len(input_ids) != 200:\n",
        "        #   print(str1,len(input_ids))\n",
        "        return [input_ids, input_masks, input_segments]\n",
        "    \n",
        "    input_ids,input_masks,input_segments = return_id(instance, 'longest_first', max_sequence_length)\n",
        "    return [input_ids, input_masks,input_segments]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AC_BzOX5JKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_input_arrays(df,columns,tokenizer,max_sequence_length):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for instance in tqdm(df[columns]):\n",
        "        \n",
        "        ids, masks, segments = \\\n",
        "        _convert_to_transformer_inputs(str(instance), tokenizer, max_sequence_length)\n",
        "        \n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "    # print(input_ids)\n",
        "\n",
        "    return input_ids, input_masks, input_segments\n",
        "\n",
        "def compute_output_arrays(df,columns):\n",
        "    return np.asarray(df[columns].astype(int) + 1)\n",
        "\n",
        "def data_loader(input_ids,input_masks,input_segments,label_ids):\n",
        "    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "    all_input_mask = torch.tensor(input_masks, dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor(input_segments, dtype=torch.long)\n",
        "    all_label = torch.tensor(label_ids, dtype=torch.long)\n",
        "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n",
        "    return train_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHVibWms-TLT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2ac671ed-a1fa-4131-a7b5-b1c3bd0cdbb3"
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet/spiece.model\")\n",
        "t_input_ids, t_input_masks, t_input_segments = compute_input_arrays(train_0,input_categories,tokenizer,MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "t_label_ids = compute_output_arrays(train_0, output_categories)\n",
        "train_dataloader = data_loader(t_input_ids, t_input_masks, t_input_segments,t_label_ids)\n",
        "v_input_ids, v_input_masks, v_input_segments = compute_input_arrays(val_0,input_categories,tokenizer,MAX_SEQUENCE_LENGTH)\n",
        "v_label_ids = compute_output_arrays(val_0, output_categories)\n",
        "val_dataloader = data_loader(v_input_ids, v_input_masks, v_input_segments,v_label_ids)\n",
        "device = torch.device(\"cuda:0\")\n",
        "# bert_model = BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path='bert-base-chinese', num_labels=3)\n",
        "xlnet_model = XLNetForSequenceClassification.from_pretrained(\"xlnet/\", num_labels=3)\n",
        "param_optimizer = list(xlnet_model.named_parameters())\n",
        "# hack to remove pooler, which is not used\n",
        "# thus it produce None grad that break apex\n",
        "param_optimizer = [n for n in param_optimizer]\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "xlnet_model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 79931/79931 [00:19<00:00, 4171.97it/s]\n",
            "100%|██████████| 19982/19982 [00:04<00:00, 4403.83it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLNetForSequenceClassification(\n",
              "  (transformer): XLNetModel(\n",
              "    (word_embedding): Embedding(32000, 768)\n",
              "    (layer): ModuleList(\n",
              "      (0): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (7): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (sequence_summary): SequenceSummary(\n",
              "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "    (first_dropout): Identity()\n",
              "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (logits_proj): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MatTjNrj_e_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    i = 0\n",
        "    \n",
        "    for input_ids, segment_ids,input_mask,label_ids in iterator:\n",
        "        input_ids = input_ids.to(device)\n",
        "        input_mask = input_mask.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        label_ids = label_ids.to(device)\n",
        "        loss = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, labels=label_ids)\n",
        "        model.zero_grad()\n",
        "        # print(loss[0])\n",
        "        # print(\"-----\")\n",
        "        # y_pred_label = loss[1].cpu()\n",
        "        # loss = F.cross_entropy(y_pred, label_ids)\n",
        "        epoch_loss += loss[0]\n",
        "        y_pred_c = loss[1].argmax(dim=1).cpu()\n",
        "        # print(y_pred_c)\n",
        "        label_ids_c = label_ids.cpu()\n",
        "        # print(label_ids_c)\n",
        "        if i %300 == 0:\n",
        "          # print(\"i\",i,\"loss\",loss[0].cpu())\n",
        "\n",
        "          print(\"i\",i,\"loss\",loss[0].cpu(),\"train acc:\",accuracy_score(y_pred_c,label_ids_c),\"train rec:\",recall_score(y_pred_c,label_ids_c,average='macro'),\"train f1\",f1_score(y_pred_c,label_ids_c,average='macro'))\n",
        "        loss[0].backward()\n",
        "        optimizer.step() \n",
        "        i += 1\n",
        "    end = time.time()\n",
        "    runtime = end-start\n",
        "    print('time: %.2f' , runtime)\n",
        "    return epoch_loss / len(iterator)\n",
        "def deval(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    abblist = []\n",
        "    n = 0\n",
        "    f1 = 0\n",
        "    acc = 0\n",
        "    rec = 0\n",
        "    with torch.no_grad():\n",
        "      for input_ids,  segment_ids,input_mask,label_ids in iterator:\n",
        "        \n",
        "        n += 1\n",
        "        input_ids = input_ids.to(device)\n",
        "        input_mask = input_mask.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        output = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask)\n",
        "        # print(output)\n",
        "        y_pred_label = output[0].argmax(dim=1).cpu()\n",
        "        acc += accuracy_score(y_pred_label,label_ids)\n",
        "        rec += recall_score(y_pred_label,label_ids,average='macro')\n",
        "        f1 += f1_score(y_pred_label,label_ids,average='macro')\n",
        "      print(\"train acc :\",acc/n, \"rec:\",rec/n,\"f1:\",f1/n,'n:',n)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmt1Sul5ABNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "8838cde0-ab7a-498c-ac94-914778f98b69"
      },
      "source": [
        "stat = 0 \n",
        "for i in range(epochs):\n",
        "    train_loss = train(xlnet_model, train_dataloader, optimizer, criterion, device) \n",
        "    lss = \"xlnet_model/p100_xlnet_\"+str(i)+\"_.pk1\"\n",
        "    if i == 0 :\n",
        "      stat = train_loss\n",
        "    if i != 0 :\n",
        "      if stat-train_loss < 0.005 :\n",
        "        break\n",
        "      if stat-train_loss < 0 :\n",
        "        break\n",
        "    torch.save(xlnet_model.state_dict(), lss)\n",
        "    print(\"train loss: \", train_loss)\n",
        "    deval(xlnet_model, val_dataloader, criterion, device)\n",
        "    # deval(xlnet_model, train_dataloader, criterion, device)\n",
        "torch.save(xlnet_model.state_dict(), \"xlnet/params_bertend.pk1\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i 0 loss tensor(1.0825, grad_fn=<CopyBackwards>) train acc: 0.484375 train rec: 0.34595959595959597 train f1 0.3279778035875597\n",
            "i 300 loss tensor(0.5132, grad_fn=<CopyBackwards>) train acc: 0.75 train rec: 0.7260452961672473 train f1 0.6866873065015479\n",
            "i 600 loss tensor(0.6601, grad_fn=<CopyBackwards>) train acc: 0.71875 train rec: 0.6746031746031745 train f1 0.6476327872921321\n",
            "i 900 loss tensor(0.7097, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.7420634920634921 train f1 0.7277777777777779\n",
            "i 1200 loss tensor(0.6036, grad_fn=<CopyBackwards>) train acc: 0.734375 train rec: 0.7641196013289037 train f1 0.6901748040988548\n",
            "time: %.2f 3074.321467399597\n",
            "train loss:  tensor(0.6108, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train acc : 0.7532376768598813 rec: 0.7223647299998486 f1: 0.7195447134773881 n: 313\n",
            "i 0 loss tensor(0.5552, grad_fn=<CopyBackwards>) train acc: 0.71875 train rec: 0.6887125220458553 train f1 0.7049247049247049\n",
            "i 300 loss tensor(0.4544, grad_fn=<CopyBackwards>) train acc: 0.796875 train rec: 0.7195767195767195 train f1 0.7045576146699742\n",
            "i 600 loss tensor(0.4945, grad_fn=<CopyBackwards>) train acc: 0.8125 train rec: 0.7390572390572391 train f1 0.7323232323232324\n",
            "i 900 loss tensor(0.5348, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.7527993109388458 train f1 0.7255689424364123\n",
            "i 1200 loss tensor(0.5731, grad_fn=<CopyBackwards>) train acc: 0.734375 train rec: 0.7085137085137084 train f1 0.6884952585435679\n",
            "time: %.2f 3081.0976371765137\n",
            "train loss:  tensor(0.5269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "train acc : 0.7595704016430854 rec: 0.7458146165118041 f1: 0.7140123829379131 n: 313\n",
            "i 0 loss tensor(0.4402, grad_fn=<CopyBackwards>) train acc: 0.828125 train rec: 0.8166833166833167 train f1 0.815873015873016\n",
            "i 300 loss tensor(0.4221, grad_fn=<CopyBackwards>) train acc: 0.84375 train rec: 0.8856209150326798 train f1 0.8287037037037037\n",
            "i 600 loss tensor(0.4046, grad_fn=<CopyBackwards>) train acc: 0.828125 train rec: 0.8020202020202021 train f1 0.8117862529627237\n",
            "i 900 loss tensor(0.4303, grad_fn=<CopyBackwards>) train acc: 0.78125 train rec: 0.6992485833949248 train f1 0.7275823532542175\n",
            "i 1200 loss tensor(0.3724, grad_fn=<CopyBackwards>) train acc: 0.796875 train rec: 0.7487179487179487 train f1 0.750642113969788\n",
            "time: %.2f 3078.013085603714\n",
            "train loss:  tensor(0.4690, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "train acc : 0.761923779096303 rec: 0.7324293571682562 f1: 0.7290822225482199 n: 313\n",
            "i 0 loss tensor(0.3583, grad_fn=<CopyBackwards>) train acc: 0.890625 train rec: 0.8880341880341881 train f1 0.8776334776334777\n",
            "i 300 loss tensor(0.4456, grad_fn=<CopyBackwards>) train acc: 0.828125 train rec: 0.7944444444444444 train f1 0.7863350359019913\n",
            "i 600 loss tensor(0.4021, grad_fn=<CopyBackwards>) train acc: 0.859375 train rec: 0.7928571428571428 train f1 0.8171918825276356\n",
            "i 900 loss tensor(0.3322, grad_fn=<CopyBackwards>) train acc: 0.890625 train rec: 0.8533875338753388 train f1 0.8792949576082106\n",
            "i 1200 loss tensor(0.5757, grad_fn=<CopyBackwards>) train acc: 0.796875 train rec: 0.8510520487264673 train f1 0.7685394921903916\n",
            "time: %.2f 3074.9561200141907\n",
            "train loss:  tensor(0.4034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "train acc : 0.7541362391602008 rec: 0.7211057793680982 f1: 0.7228000328227326 n: 313\n",
            "i 0 loss tensor(0.3677, grad_fn=<CopyBackwards>) train acc: 0.875 train rec: 0.8634878193701723 train f1 0.8667222622341836\n",
            "i 300 loss tensor(0.3219, grad_fn=<CopyBackwards>) train acc: 0.875 train rec: 0.8501683501683502 train f1 0.8736631016042781\n",
            "i 600 loss tensor(0.1899, grad_fn=<CopyBackwards>) train acc: 0.9375 train rec: 0.9518207282913164 train f1 0.933254109724698\n",
            "i 900 loss tensor(0.4100, grad_fn=<CopyBackwards>) train acc: 0.828125 train rec: 0.8509803921568627 train f1 0.8432675892630126\n",
            "i 1200 loss tensor(0.4706, grad_fn=<CopyBackwards>) train acc: 0.859375 train rec: 0.8651084010840108 train f1 0.834187801521407\n",
            "time: %.2f 3075.946867465973\n",
            "train loss:  tensor(0.3310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "train acc : 0.7466482199908717 rec: 0.7124983797041882 f1: 0.7155728949767062 n: 313\n",
            "i 0 loss tensor(0.2577, grad_fn=<CopyBackwards>) train acc: 0.90625 train rec: 0.9117647058823529 train f1 0.9008098037948785\n",
            "i 300 loss tensor(0.3006, grad_fn=<CopyBackwards>) train acc: 0.890625 train rec: 0.8941176470588236 train f1 0.8868695316520844\n",
            "i 600 loss tensor(0.2446, grad_fn=<CopyBackwards>) train acc: 0.890625 train rec: 0.8702865761689291 train f1 0.8835960344365614\n",
            "i 900 loss tensor(0.1903, grad_fn=<CopyBackwards>) train acc: 0.921875 train rec: 0.9057889822595705 train f1 0.9134397175894241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8c8rC2fAUTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}